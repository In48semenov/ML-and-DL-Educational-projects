{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blank__08_rnn_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WmWCBWxrBUB3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKMq7dp2W15Y",
        "outputId": "500441e7-25f2-4f0d-b44e-4f3701463f8a"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# from pytorchtools_st import EarlyStopping\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBkqaK5bawXN",
        "outputId": "ec2e3197-646b-4409-e7bf-295e4febc119"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qOQwNlZbFiO",
        "outputId": "17123e05-c337-4d6c-d12f-2b9aa6f5b68c"
      },
      "source": [
        "cd drive/MyDrive/datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmWCBWxrBUB3"
      },
      "source": [
        "## 1. Генерирование русских имен при помощи RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990obDBwCC7V"
      },
      "source": [
        "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
        "\n",
        "1.1 На основе файла name_rus.txt создайте датасет.\n",
        "  * Учтите, что имена могут иметь различную длину\n",
        "  * Добавьте 4 специальных токена: \n",
        "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
        "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
        "    * `<SOS>` для обозначения начала последовательности;\n",
        "    * `<EOS>` для обозначения конца последовательности.\n",
        "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
        "    * в начало последовательности добавьте токен `<SOS>`;\n",
        "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
        "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ. \n",
        "  \n",
        "  Пример:\n",
        "  ```\n",
        "  s = 'The cat sat on the mat'\n",
        "  # преобразуем в индексы\n",
        "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  # получаем x и y (__getitem__)\n",
        "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
        "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  ```\n",
        "\n",
        "1.2 Создайте и обучите модель для генерации фамилии.\n",
        "\n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
        "  * Используйте рекуррентные слои;\n",
        "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
        "  * Примерная схема реализации метода `forward`:\n",
        "  ```\n",
        "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
        "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size] \n",
        "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
        "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
        "  ```\n",
        "\n",
        "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
        "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
        "  * Начальное скрытое состояние RNN `h_t = None`;\n",
        "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
        "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
        "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
        "\n",
        "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn8r4SpdrvF1",
        "outputId": "bbd91012-c598-41dc-f0f9-d59a97f091a1"
      },
      "source": [
        "weights = torch.tensor([0, 10, 40, 1], dtype=torch.float) # create a tensor of weights\n",
        "torch.multinomial(weights, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 1, 3, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "PXFHV0CKEqzC",
        "outputId": "0bb9c572-0f7b-4376-a523-2745373cdd52"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/datasets/name_rus.txt', encoding = 'cp1251', header = None, names = ['Name_rus'])\n",
        "display(df.head())\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name_rus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>авдокея</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>авдоким</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>авдоня</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>авдотька</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>авдотьюшка</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Name_rus\n",
              "0     авдокея\n",
              "1     авдоким\n",
              "2      авдоня\n",
              "3    авдотька\n",
              "4  авдотьюшка"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1988, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfrqpUZmLcq9"
      },
      "source": [
        "class Vocab:\n",
        "  def __init__(self, data):\n",
        "    tokens = set()\n",
        "    for name in data:\n",
        "      tokens.update(name)\n",
        "\n",
        "    self.max_seq_len = max([len(name) for name in data]) + 2 # Т.к. будут добавлены токены <SOS> and <EOS>\n",
        "\n",
        "    self.idx_to_token = {(idx + 4): token for idx, token in enumerate(tokens)}\n",
        "    self.idx_to_token[0] = '<PAD>' # для дополнения последовательности до нужной длины\n",
        "    self.idx_to_token[1] = '<UNK>' # для корректной обработки ранее не встречавшихся токенов\n",
        "    self.idx_to_token[2] = '<SOS>' # для обозначения начала последовательности\n",
        "    self.idx_to_token[3] = '<EOS>' # для обозначения конца последовательности\n",
        "    self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n",
        "    self.vocab_len = len(self.idx_to_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8cEWQiqLclT"
      },
      "source": [
        "class NameRusDataset(Dataset):\n",
        "  def __init__(self, X, vocab: Vocab):\n",
        "    self.X = X\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def vectorize(self, name):\n",
        "    name_t = torch.zeros(self.vocab.max_seq_len, dtype = int)\n",
        "    \n",
        "    name_t[0] =  self.vocab.token_to_idx['<SOS>']\n",
        "    for idx, char in enumerate(name, 1): # начинаем с единицы, т.к. токен 0-ая позиция занята <SOS> \n",
        "      if char in self.vocab.token_to_idx:\n",
        "        name_t[idx] = self.vocab.token_to_idx[char]\n",
        "      else:\n",
        "        name_t[idx] = self.vocab.token_to_idx['<UNK>']\n",
        "        \n",
        "    name_t[idx + 1] = self.vocab.token_to_idx['<EOS>']\n",
        "\n",
        "    return name_t\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    vector = self.vectorize(self.X[idx])\n",
        "    return vector[:-1], vector[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNLwRRSiLcoR",
        "outputId": "bb313770-0104-4f26-8b28-3b10ec9a0fab"
      },
      "source": [
        "vocab = Vocab(df['Name_rus'])\n",
        "vocab.max_seq_len, vocab.vocab_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 34)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q63SvsXoLcip",
        "outputId": "e319bc20-8287-4832-f7b6-b033d4ad9e7c"
      },
      "source": [
        "dataset = NameRusDataset(list(df['Name_rus']), vocab)\n",
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 2,  6,  8, 13, 15, 25,  4, 10,  3,  0,  0,  0,  0,  0]),\n",
              " tensor([ 6,  8, 13, 15, 25,  4, 10,  3,  0,  0,  0,  0,  0,  0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3ofALH0COXr"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, vocab_size: int, embedding_dim: int, hidden_size: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    self.rnn = nn.GRU(input_size = embedding_dim, \n",
        "                      hidden_size = hidden_size,\n",
        "                      batch_first = True) \n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "    self.dropout = nn.Dropout(p=0.25)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    # input_tensor: batch_size x seq_len\n",
        "    embedded = self.embeddings(input_tensor)\n",
        "    batch_size, seq_len, embedding_size = embedded.shape\n",
        "    # embedded: batch_size x seq_len x embedding_size\n",
        "    output, hidden = self.rnn(embedded)\n",
        "    # output: batch_size x seq_len x hidden_size\n",
        "    # hidden: 1 x batch_size x hidden_size\n",
        "    output = output.reshape(batch_size*seq_len, self.hidden_size)\n",
        "    output = self.dropout(output)\n",
        "    # output: batch_size*seq_len x hidden_size\n",
        "    output = self.fc(output)\n",
        "    # output: batch_size*seq_len x vocab_size\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CzjX-QVPJRaI"
      },
      "source": [
        "dataset_train = NameRusDataset(list(df['Name_rus']), vocab)\n",
        "dataloader = DataLoader(dataset_train, batch_size = 32, shuffle = True, drop_last = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV0GMnMqLUmH"
      },
      "source": [
        "def generate(model, vocab: Vocab):\n",
        "  indices = [2]\n",
        "  h_t = None\n",
        "  sample_size = vocab.max_seq_len\n",
        "\n",
        "  model.eval()\n",
        "  for step in range(sample_size):\n",
        "    x_t = torch.LongTensor([indices[step]]).unsqueeze(dim=0).to(device=device)\n",
        "\n",
        "    # print(x_t.shape)\n",
        "    x_emp = model.embeddings(x_t)\n",
        "    # print(x_emp.shape)\n",
        "    out_t, h_t = model.rnn(x_emp, h_t)\n",
        "    out_t = out_t.squeeze(dim=1)\n",
        "    # print(out_t.shape)\n",
        "    predictions = model.fc(out_t).softmax(dim=1)\n",
        "    # print(predictions.shape)\n",
        "    char_idx = torch.multinomial(predictions, num_samples=1)\n",
        "    # print(char_idx.shape)\n",
        "    if char_idx == 3:\n",
        "      break\n",
        "    indices.append(char_idx.item())\n",
        "  return ''.join(vocab.idx_to_token[token_idx] for token_idx in indices if token_idx != 2).capitalize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jN34iEdHH8h",
        "outputId": "89911dfe-744e-48e5-9a3f-49dc742f065a"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CharRNN(vocab_size = vocab.vocab_len,\n",
        "                hidden_size=32,\n",
        "                embedding_dim=32).to(device)\n",
        "criteriation = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "n_epochs = 20\n",
        "for epoch in range(n_epochs):\n",
        "  epoch_loss = 0\n",
        "  model.train()\n",
        "  for X_batch, y_barch in dataloader:\n",
        "    X_batch = X_batch.to(device=device)\n",
        "    y_barch = y_barch.to(device=device) # batch_size x seq_len\n",
        "    predictions = model(X_batch)\n",
        "    # predictions: batch_size*seq_len x vocab_size\n",
        "    y_barch = y_barch.reshape(-1)\n",
        "    # y_batch: batch_size*seq_len\n",
        "    loss = criteriation(predictions, y_barch)\n",
        "    loss.backward()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  model.eval()\n",
        "  print(f\"Epoch #{epoch} loss = {epoch_loss / len(dataloader)} Example: {generate(model)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #0 loss = 3.4550187472374208 Example: <sos>м<sos>оз<unk>нтхбууюухыэртгю\n",
            "Example:\n",
            "Epoch #1 loss = 3.381063195966905 Example: <sos>впхэжабырен<pad>вйьаь<pad>йа\n",
            "Example:\n",
            "Epoch #2 loss = 3.304799537504873 Example: <sos>агцдфьяечрюнвгьодбаю\n",
            "Example:\n",
            "Epoch #3 loss = 3.2162345186356576 Example: <sos>\n",
            "Example:\n",
            "Epoch #4 loss = 3.116611865258986 Example: <sos>гйутзьююцпт<pad>уттгчяоб\n",
            "Example:\n",
            "Epoch #5 loss = 3.0133990741545156 Example: <sos>зеб<unk>кффкьена\n",
            "Example:\n",
            "Epoch #6 loss = 2.9213622500819545 Example: <sos>епчзлбеюлшжьго\n",
            "Example:\n",
            "Epoch #7 loss = 2.8509089716019167 Example: <sos>о\n",
            "Example:\n",
            "Epoch #8 loss = 2.798775311439268 Example: <sos>д\n",
            "Example:\n",
            "Epoch #9 loss = 2.7576251106877483 Example: <sos>еоа\n",
            "Example:\n",
            "Epoch #10 loss = 2.7212421317254343 Example: <sos>снозя\n",
            "Example:\n",
            "Epoch #11 loss = 2.689467168623401 Example: <sos>фифснолнси\n",
            "Example:\n",
            "Epoch #12 loss = 2.6585454979250507 Example: <sos>врлюя\n",
            "Example:\n",
            "Epoch #13 loss = 2.630206650303256 Example: <sos>антакф\n",
            "Example:\n",
            "Epoch #14 loss = 2.602714023282451 Example: <sos>тяше\n",
            "Example:\n",
            "Epoch #15 loss = 2.577015442232932 Example: <sos>насявя\n",
            "Example:\n",
            "Epoch #16 loss = 2.5518364060309624 Example: <sos>аешлаи\n",
            "Example:\n",
            "Epoch #17 loss = 2.5265905703267744 Example: <sos>юруати<sos>сата\n",
            "Example:\n",
            "Epoch #18 loss = 2.5025430956194477 Example: <sos>мжишаспб\n",
            "Example:\n",
            "Epoch #19 loss = 2.479205089230691 Example: <sos>нилиь<pad>иркис\n",
            "Example:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJf5iaA2fOTM"
      },
      "source": [
        "## 2. Генерирование текста при помощи RNN\n",
        "\n",
        "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
        "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
        "\n",
        "2.2 На основе выбранного произведения создайте датасет. \n",
        "\n",
        "Отличия от задачи 1:\n",
        "  * Токены <SOS>, `<EOS>` и `<UNK>` можно не добавлять;\n",
        "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием).\n",
        "\n",
        "2.3 Создайте и обучите модель для генерации текста\n",
        "  * Задача ставится точно так же как в 1.2;\n",
        "  * При необходимости можете применить:\n",
        "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
        "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
        "\n",
        "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
        "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова) \n",
        "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
        "  * После этого вы генерируете строку нужной длины аналогично 1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "p2wTTvCna0Z1",
        "outputId": "9103a129-bb2a-48a6-a145-37085401382a"
      },
      "source": [
        "df = pd.DataFrame(columns = ['Part_text'])\n",
        "txt = ''\n",
        "with open('/content/drive/MyDrive/datasets/AnnaKarenina__.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    l = re.sub(r\"\\n\", \" \", line)\n",
        "    txt += ' ' + ' '.join(l.split())\n",
        "\n",
        "txt = txt.split()\n",
        "\n",
        "k = 0\n",
        "for idx in range(0, len(txt), 100):\n",
        "  if idx + 100 <= len(txt):\n",
        "    df.loc[k, 'Part_text'] = ' '.join(txt[idx: idx + 100])\n",
        "  else:\n",
        "    df.loc[k, 'Part_text'] = ' '.join(txt[idx:])\n",
        "  k += 1\n",
        "\n",
        "display(df)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Part_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Лев Толстой Анна Каренина Роман «Широкого дыха...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>находил в новом романе Толстого «огромную псих...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>в объявлении не называть его сочинение романом...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>начиная работу, он говорил Н.Н. Страхову: «…ро...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70-е же годы, в эпоху глубокого социального кр...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2947</th>\n",
              "      <td>48). 322 …читая Шопенгауера, он подставил на м...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2948</th>\n",
              "      <td>катехизисного изложения учения о церкви» и «Мы...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2949</th>\n",
              "      <td>и в одной из последних повестей – «После бала»...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2950</th>\n",
              "      <td>332 Т.А. Кузминская. Моя жизнь дома и в Ясной ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2951</th>\n",
              "      <td>54. 343 А.Н. Мошин. Ясная Поляна и Васильевка....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2952 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Part_text\n",
              "0     Лев Толстой Анна Каренина Роман «Широкого дыха...\n",
              "1     находил в новом романе Толстого «огромную псих...\n",
              "2     в объявлении не называть его сочинение романом...\n",
              "3     начиная работу, он говорил Н.Н. Страхову: «…ро...\n",
              "4     70-е же годы, в эпоху глубокого социального кр...\n",
              "...                                                 ...\n",
              "2947  48). 322 …читая Шопенгауера, он подставил на м...\n",
              "2948  катехизисного изложения учения о церкви» и «Мы...\n",
              "2949  и в одной из последних повестей – «После бала»...\n",
              "2950  332 Т.А. Кузминская. Моя жизнь дома и в Ясной ...\n",
              "2951  54. 343 А.Н. Мошин. Ясная Поляна и Васильевка....\n",
              "\n",
              "[2952 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2952, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd_tbKXlD8DK"
      },
      "source": [
        "df.to_csv('/content/drive/MyDrive/datasets/AnnaKarenina_conclusion.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "BsC3ZkckKEe1",
        "outputId": "b7387dd1-b5a3-4ccc-da66-e6a1d9a62bc9"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/datasets/AnnaKarenina_conclusion.csv')\n",
        "df"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Part_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Лев Толстой Анна Каренина Роман «Широкого дыха...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>находил в новом романе Толстого «огромную псих...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>в объявлении не называть его сочинение романом...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>начиная работу, он говорил Н.Н. Страхову: «…ро...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>70-е же годы, в эпоху глубокого социального кр...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2947</th>\n",
              "      <td>2947</td>\n",
              "      <td>48). 322 …читая Шопенгауера, он подставил на м...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2948</th>\n",
              "      <td>2948</td>\n",
              "      <td>катехизисного изложения учения о церкви» и «Мы...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2949</th>\n",
              "      <td>2949</td>\n",
              "      <td>и в одной из последних повестей – «После бала»...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2950</th>\n",
              "      <td>2950</td>\n",
              "      <td>332 Т.А. Кузминская. Моя жизнь дома и в Ясной ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2951</th>\n",
              "      <td>2951</td>\n",
              "      <td>54. 343 А.Н. Мошин. Ясная Поляна и Васильевка....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2952 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                          Part_text\n",
              "0              0  Лев Толстой Анна Каренина Роман «Широкого дыха...\n",
              "1              1  находил в новом романе Толстого «огромную псих...\n",
              "2              2  в объявлении не называть его сочинение романом...\n",
              "3              3  начиная работу, он говорил Н.Н. Страхову: «…ро...\n",
              "4              4  70-е же годы, в эпоху глубокого социального кр...\n",
              "...          ...                                                ...\n",
              "2947        2947  48). 322 …читая Шопенгауера, он подставил на м...\n",
              "2948        2948  катехизисного изложения учения о церкви» и «Мы...\n",
              "2949        2949  и в одной из последних повестей – «После бала»...\n",
              "2950        2950  332 Т.А. Кузминская. Моя жизнь дома и в Ясной ...\n",
              "2951        2951  54. 343 А.Н. Мошин. Ясная Поляна и Васильевка....\n",
              "\n",
              "[2952 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQa-anuIyKQ1"
      },
      "source": [
        "class Vocab:\n",
        "  def __init__(self, data):\n",
        "    tokens = [] # Все слова в словаре\n",
        "    for txt in data:\n",
        "      # for sent in sent_tokenize(txt): # лексемазация предложения\n",
        "      for word in word_tokenize(txt): # лексемазация слов\n",
        "        tokens.append(word)\n",
        "\n",
        "    tokens = set(tokens)\n",
        "\n",
        "    self.max_seq_len = max([len(word_tokenize(txt)) for txt in data]) + 1\n",
        "\n",
        "    self.idx_to_token = {(idx + 2): token for idx, token in enumerate(tokens)}\n",
        "    self.idx_to_token[0] = '<PAD>' # для дополнения последовательности до нужной длины\n",
        "    self.idx_to_token[1] = '<SOS>' # для обозначения начала последовательности\n",
        "    self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()} # токен слово\n",
        "    self.vocab_len = len(self.idx_to_token)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTgKC4s82xR8",
        "outputId": "cd18f433-020f-4c0f-ab18-19609ccf569d"
      },
      "source": [
        "vocab = Vocab(df['Part_text'])\n",
        "vocab.vocab_len, vocab.max_seq_len"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39103, 184)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PO03_6U2yhL"
      },
      "source": [
        "class NameRusDataset(Dataset):\n",
        "  def __init__(self, X: list, vocab: Vocab):\n",
        "    self.X = X\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def vectorize(self, seq):\n",
        "    seq_t = torch.zeros(self.vocab.max_seq_len, dtype = int)\n",
        "    \n",
        "    seq_t[0] =  self.vocab.token_to_idx['<SOS>']\n",
        "    for idx, char in enumerate(word_tokenize(seq), 1): # начинаем с единицы, т.к. токен 0-ая позиция занята <SOS> \n",
        "      seq_t[idx] = self.vocab.token_to_idx[char]\n",
        "\n",
        "    return seq_t\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    vector = self.vectorize(self.X[idx])\n",
        "    return vector[:-1], vector[1:]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aPcH8nS5bid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe6c085-9984-4fcb-83a1-b0f4ad72e00a"
      },
      "source": [
        "dataset = NameRusDataset(list(df['Part_text']), vocab)\n",
        "dataset[0]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([    1, 23784,  5271, 12473, 24554, 23959,  6124,    66, 39030, 24274,\n",
              "          6124, 12473, 24554, 24274, 35254, 31099,  6124, 28301, 27794, 24274,\n",
              "         26068, 14612, 22515, 31423, 24056, 11573, 10447, 29784, 12822, 26134,\n",
              "         36064, 38938, 26086, 31746, 12652, 11743, 28254, 24797, 26068,  4201,\n",
              "         35758, 12776,  4335, 30067, 16198,  3569, 16198, 18589, 27889, 25473,\n",
              "          6124, 17309, 12822, 27042, 31423, 18986, 13186, 11466, 36919, 31423,\n",
              "           913, 12822, 27042, 31423, 18986, 16535, 19794, 24797, 12822, 23492,\n",
              "         31423, 25318, 17309, 11132, 16863, 17687, 30222, 24274, 27510, 19847,\n",
              "         31423, 35819, 24047, 26068, 15354, 13764, 21615, 15729, 18675, 10103,\n",
              "         18878, 17438, 28617, 27510, 12998, 31423, 22114, 24047,  3708,  1137,\n",
              "         14213,  6124,  4643, 37465, 33895, 31423, 19064, 37465,  5341, 26068,\n",
              "         22140, 10910, 10299, 31423,  5383, 14526, 31423, 27302, 11531, 38938,\n",
              "         33720,  1553, 22696, 12822, 17547, 18414, 22103,  5713, 14442, 27644,\n",
              "          1534,  1553, 24274, 15354, 14949, 21615, 26827, 26068, 31335,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0]),\n",
              " tensor([23784,  5271, 12473, 24554, 23959,  6124,    66, 39030, 24274,  6124,\n",
              "         12473, 24554, 24274, 35254, 31099,  6124, 28301, 27794, 24274, 26068,\n",
              "         14612, 22515, 31423, 24056, 11573, 10447, 29784, 12822, 26134, 36064,\n",
              "         38938, 26086, 31746, 12652, 11743, 28254, 24797, 26068,  4201, 35758,\n",
              "         12776,  4335, 30067, 16198,  3569, 16198, 18589, 27889, 25473,  6124,\n",
              "         17309, 12822, 27042, 31423, 18986, 13186, 11466, 36919, 31423,   913,\n",
              "         12822, 27042, 31423, 18986, 16535, 19794, 24797, 12822, 23492, 31423,\n",
              "         25318, 17309, 11132, 16863, 17687, 30222, 24274, 27510, 19847, 31423,\n",
              "         35819, 24047, 26068, 15354, 13764, 21615, 15729, 18675, 10103, 18878,\n",
              "         17438, 28617, 27510, 12998, 31423, 22114, 24047,  3708,  1137, 14213,\n",
              "          6124,  4643, 37465, 33895, 31423, 19064, 37465,  5341, 26068, 22140,\n",
              "         10910, 10299, 31423,  5383, 14526, 31423, 27302, 11531, 38938, 33720,\n",
              "          1553, 22696, 12822, 17547, 18414, 22103,  5713, 14442, 27644,  1534,\n",
              "          1553, 24274, 15354, 14949, 21615, 26827, 26068, 31335,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2LoKw745wlf"
      },
      "source": [
        "class GenerizeText(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, residual = 'RNN', pretrained_embeddings = []):\n",
        "    super().__init__()\n",
        "\n",
        "    if len(pretrained_embeddings) > 0:\n",
        "      self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
        "    else:\n",
        "      self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    \n",
        "    self.residual = residual.lower()\n",
        "\n",
        "    if self.residual == 'rnn':\n",
        "      self.rl = nn.RNN(input_size = embedding_dim, \n",
        "                        hidden_size = hidden_size,\n",
        "                        num_layers = 2, \n",
        "                        nonlinearity = 'relu',\n",
        "                        batch_first = True,\n",
        "                        dropout = 0.25)\n",
        "      \n",
        "    elif self.residual == 'lstm':\n",
        "      self.rl = nn.LSTM(input_size = embedding_dim, \n",
        "                        hidden_size = hidden_size,\n",
        "                        num_layers = 2,\n",
        "                        batch_first = True)  \n",
        "                        # dropout = 0.5)    \n",
        "\n",
        "    elif self.residual == 'gru':\n",
        "      self.rl = nn.GRU(input_size = embedding_dim, \n",
        "                        hidden_size = hidden_size,\n",
        "                        num_layers = 2, \n",
        "                        batch_first = True) \n",
        "                        # dropout = 0.5)\n",
        "\n",
        "    # self.fc = nn.Linear(hidden_size, n_classes)\n",
        "    self.fc = nn.Sequential(nn.Linear(hidden_size, vocab_size))\n",
        "    \n",
        "  def forward(self, input):\n",
        "    embed = self.embeddings(input)\n",
        "    # print('Embed: ', embed.shape)\n",
        "    if self.residual == 'lstm':\n",
        "      out, (hn, cn) = self.rl(embed)\n",
        "      # out = hn[0]\n",
        "    else:\n",
        "      out, hidden = self.rl(embed)\n",
        "      # out = hidden[0]\n",
        "    # print('Output RNN: ', out.shape)\n",
        "    out = torch.reshape(out, (-1, out.shape[-1]))\n",
        "    # print('Output reshape: ', out.size())\n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHvJwR7sFTvP"
      },
      "source": [
        "def generateText(model, start_seq: list, vocab: Vocab, device: str):\n",
        "  indices = [2]\n",
        "  for word in start_seq:\n",
        "    indices.append(vocab.token_to_idx[word])\n",
        "\n",
        "  h_t = None\n",
        "  sample_size = vocab.max_seq_len\n",
        "\n",
        "  for step in range(sample_size):\n",
        "    x_t = torch.LongTensor([indices[step]]).unsqueeze(dim=0).to(device=device)\n",
        "    x_emp = model.embeddings(x_t)\n",
        "    out_t, h_t = model.rl(x_emp, h_t)\n",
        "    out_t = out_t.squeeze(dim=1)\n",
        "    predictions = model.fc(out_t).softmax(dim=1)\n",
        "    char_idx = torch.multinomial(predictions, num_samples=1)\n",
        "    if step >= 2:\n",
        "      indices.append(char_idx.item())\n",
        "  \n",
        "  text = ''\n",
        "  for idx in range(0, len(indices), 15):\n",
        "    if idx + 15 < len(indices):\n",
        "      text += ' '.join(vocab.idx_to_token[token_idx] for token_idx in indices[idx: idx + 15] if token_idx != 2) + '\\n'\n",
        "    else:\n",
        "      text += ' '.join(vocab.idx_to_token[token_idx] for token_idx in indices[idx:] if token_idx != 2) + '\\n'\n",
        "  return text #' '.join(vocab.idx_to_token[token_idx] for token_idx in indices if token_idx != 2)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7tyY64J5wZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "22c344d7-6237-4f40-922e-355a3c1ebedc"
      },
      "source": [
        "dataset = NameRusDataset(list(df['Part_text']), vocab)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=True, drop_last=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = 'cpu'\n",
        "model = GenerizeText(vocab_size=vocab.vocab_len,\n",
        "                     hidden_size=128,\n",
        "                     embedding_dim=100,\n",
        "                     residual='rnn').to(device)\n",
        "\n",
        "criteriation = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=5*10**(-4))\n",
        "\n",
        "loss_history = list()\n",
        "epochs_history = list()\n",
        "\n",
        "n_epochs = 20 + 1\n",
        "\n",
        "for epoch in range(1, n_epochs):\n",
        "  epoch_loss = 0\n",
        "  model.train()\n",
        "  for X_batch, y_batch in dataloader:\n",
        "    X_batch, y_batch = X_batch.to(device=device), y_batch.to(device=device) # y_barch: batch_size x seq_len\n",
        "    predictions = model(X_batch)\n",
        "    # predictions: batch_size*seq_len x vocab_size\n",
        "    y_batch = y_batch.reshape(-1)\n",
        "    # y_batch: batch_size*seq_len\n",
        "    loss = criteriation(predictions, y_batch)\n",
        "    loss.backward()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    optimizer.step()\n",
        "    clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  loss_history.append(epoch_loss/len(dataloader))\n",
        "  epochs_history.append(epoch)\n",
        "\n",
        "  model.eval()\n",
        "  print(f\"Epoch {epochs_history[-1]}/{n_epochs-1} loss = {loss_history[-1]} Example:\\n {generateText(model, start_seq=['Ай', 'Рядовой', 'Моргенштерн'], vocab=vocab, device=device)}\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-b11039fa4a9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# y_barch: batch_size x seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m# predictions: batch_size*seq_len x vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-99f5d233c8d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# print('Output reshape: ', out.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 78.69 GiB (GPU 0; 14.76 GiB total capacity; 1.75 GiB already allocated; 11.10 GiB free; 2.62 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9wl1Z2v2yRm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}